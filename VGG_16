# import relevant libraries
import torch
import torchvision
from torchvision import transforms, datasets
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.data as data
import matplotlib.pyplot as plt
import torch.optim as optim


# VGG16 architecture - each layer represented by number of filters or max pooling used
VGG = [64, 64, 'max_pool', 128, 128, 'max_pool', 256, 256, 256, 'max_pool', 512, 512, 512, 'max_pool',
       512, 512, 512, 'max_pool']


class VGGNet(nn.Module):
    def __init__(self, in_channels=3, num_classes=2):
        super(VGGNet, self).__init__()
        self.in_channels = in_channels
        self.conv_layers = self.generate_conv_layers(VGG)

        self.fcs = nn.Sequential(
            # expecting a (7, 7, 512) feature map after the stack of convolution and max pooling layers
            nn.Linear(512 * 7 * 7, 4096),
            nn.ReLU(),
            nn.Dropout(p=0.5),  # helps reduce overfitting
            nn.Linear(4096, 4096),
            nn.ReLU(),
            nn.Dropout(p=0.5),
            nn.Linear(4096, num_classes)  # fully connected layer
        )

    # Data moves through layers in a forward propagation
    def forward(self, x):
        x = self.conv_layers(x)
        x = x.reshape(x.shape[0], -1)  # flattens feature map
        x = self.fcs(x)

        return x

    # forming the convolution layers and max pooling layers
    def generate_conv_layers(self, architecture):
        vgg_layers = []
        in_channels = self.in_channels

        for channels in architecture:

            if channels == 'max_pool':
                vgg_layers += [nn.MaxPool2d(kernel_size=2, stride=2)]
            else:
                out_channels = channels

                vgg_layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,
                               kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(channels), nn.ReLU()]
                in_channels = channels

        return nn.Sequential(*vgg_layers)
